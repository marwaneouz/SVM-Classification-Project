# SVM-Classification-Project
ImplÃ©mentation et analyse des SVM sur les datasets Iris et LFW 
# Support Vector Machine (SVM) - Master IMSD  
![License](https://img.shields.io/badge/license-MIT-blue.svg) ![Python](https://img.shields.io/badge/Python-3.8%2B-green.svg) ![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.0%2B-orange.svg)

> **RÃ©alisÃ© par :** Marwane Ouzaina  
> **Encadrant :** Rapport SVM  
> **Master IMSD - UniversitÃ© Ibn Zohr, FacultÃ© de Ouarzazate**  
> **AnnÃ©e Universitaire : 2024â€“2025**

---

## ğŸ“˜ PrÃ©sentation

Ce projet prÃ©sente une implÃ©mentation complÃ¨te des **Support Vector Machines (SVM)** dans le cadre dâ€™un TP acadÃ©mique. Il couvre :
- Les **fondements thÃ©oriques** des SVM
- Lâ€™implÃ©mentation avec `scikit-learn`
- Des expÃ©riences sur deux datasets : **Iris** et **Labeled Faces in the Wild (LFW)**
- Lâ€™analyse des noyaux (linÃ©aire, polynomial, RBF), du paramÃ¨tre `C`, de lâ€™impact du bruit, et de lâ€™utilisation de PCA

ğŸ¯ Objectif : Comprendre comment les SVM maximisent la marge entre classes, gÃ¨rent les donnÃ©es non linÃ©aires, et rÃ©sistent (ou non) au sur-apprentissage.

ğŸ“„ Le rapport complet est disponible ici : [TP_SVM.pdf](reports/TP_SVM.pdf)

---

## ğŸ§ª RÃ©sultats ClÃ©s

### 1. SVM sur le dataset Iris
- **PrÃ©cision (noyau linÃ©aire)** : 70%
- **PrÃ©cision (noyau polynomial)** : 54%
- âœ… Le noyau linÃ©aire est plus efficace ici : les classes sont presque linÃ©airement sÃ©parables.

 

---

### 2. SVM sur le dataset LFW (Blair vs Powell)
- **PrÃ©cision (donnÃ©es Ã©quilibrÃ©es)** : 96.8%
- **PrÃ©cision (donnÃ©es dÃ©sÃ©quilibrÃ©es)** : 93.6%
- âœ… Lâ€™Ã©quilibrage des classes amÃ©liore la performance.

 

---

### 3. Impact du paramÃ¨tre C
- Un `C` faible â†’ sous-apprentissage (marge large)
- Un `C` Ã©levÃ© â†’ sur-apprentissage possible
- **Optimum autour de C = 1**

 

---

### 4. Robustesse au bruit et avec PCA
| Condition | PrÃ©cision |
|---------|----------|
| DonnÃ©es bruitÃ©es (+300 features alÃ©atoires) | 94.7% |
| AprÃ¨s PCA (50 composantes) | 87.2% |
| Avec noyau RBF | 87.2% |

ğŸ“‰ Curieusement, PCA nâ€™amÃ©liore pas les performances ici : les composantes principales ne capturent pas toute lâ€™information discriminante.

---

 
